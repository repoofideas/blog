{
  
    
        "post0": {
            "title": "GPT3, Gopher, and 부캐(Sub Account)",
            "content": "There’s been a bit of a hiatus since my last post, but the recent paper from Deepmind motivated me to write up a quick post. . Yesterday, @DeepMind shared Gopher, a language model which contains approximately twice the number of parameters of GPT3 that was published last year. The graph they shared was quite interesting and alarming 1/ pic.twitter.com/Fjw83Uzd1s . &mdash; Kris Pan (@KrisKPan) December 10, 2021 You can check the Deepmind Blog Post to see the original graph. . Looking at the rate of innovation from GPT-3 to Gopher against the human benchmark, it seems like a huge portion of white-collar labor performed by humans will be reduced by pretrained models within the next decade. When BERT was introduced in 2019 and presented groundbreaking performances, it had 340 million parameters, after 2 years, Gopher has 280 billion parameters, which is an improvement of roughly 1000x. It would be naive to expect that people would have the resources necessary to be adaptive in this increasingly scarce market for human labor. Macro solution may involve social safety net or education reform but that is way out of the scope of this post. Rather, I would like to introduce a recent trend popularized in Korea. . This may sound a bit off tangent but there’s a phrase called ‘부캐(sub-character)’ in Korean which originated in the gaming community. It’s a term when players make a second account to play more casually than their main account. Now that term has evolved to refer to people who develop their hobby or gimmick extensively on social media for engagement and potentially create a second income. This works sort of like a hedging strategy against job automation risk by being involved in multiple industries. This is not neccessarily about the income but having a platform for social engagement through their hobby/side job. I have personally been exploring a few hobbies that do not involve staring at a screen(which I do more than enough for my main job) like jazz piano. I am at no level to share it currently but I hope to show my progress shortly. If you are reading this, I encourage you to actively explore in creating a 부캐(pronounced as boo-keh). .",
            "url": "https://repoofideas.github.io/blog/machine%20learning/socioeconomics/music%20production/2021/12/09/GPT3-Gopher-and-Sub-Account.html",
            "relUrl": "/machine%20learning/socioeconomics/music%20production/2021/12/09/GPT3-Gopher-and-Sub-Account.html",
            "date": " • Dec 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Introducing My Book Reader",
            "content": "Introducing My Book Reader . Due to COVID-19, all of my courses for fall semester turned remote. Due to extreme circumstances, I have decided to temporarily move with my family. At home, my mother is suffering from xerophthalmia, which makes her difficult to see bright lights or screens. Watching her struggle made me research how many people are suffering from vision impairment: according to estimates from WHO, a staggering 2.2 billion have some type of vision impairment and this number is quickly increasing. Now, if you are accustomed to digital format, it is not difficult to have texts read to you. However, if you are like my mother, who collects quite a few books and mostly reads printed books, then vision-impairment is truly devastating. I have tested various text-reading applications in the market, and I have identified common issues: . There is no audio text-reading capabilities | If there is audio text-reading, the reading speed cannot be controlled or you have to rely on other volunteers to help you | UI is not friendly for people with impaired vision | That is where I got inspiration to work on My Book Reader, a mobile app that can read printed text. Please provide feedbacks or PR in my-book-reader if you are interested! Many thanks to KJ Huang for collaborating on this project. .",
            "url": "https://repoofideas.github.io/blog/app%20development/applied%20ml/2020/09/20/Introducing-My-Book-Reader.html",
            "relUrl": "/app%20development/applied%20ml/2020/09/20/Introducing-My-Book-Reader.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Case for Interpretable Machine Learning",
            "content": "Case for Interpretable Machine Learning . From video recommendation, autonomous vehicles to predictive medicine, machine learning(ML) systems are ubiquitous in decision making process. ML systems are often labeled as black-box models, since their complexity makes it challenging for human evaluation or understanding. Recently, Geoffrey Hinton, one of the founders of deep learning, tweeted the following: . Suppose you have cancer and you have to choose between a black box AI surgeon that cannot explain how it works but has a 90% cure rate and a human surgeon with an 80% cure rate. Do you want the AI surgeon to be illegal? . &mdash; Geoffrey Hinton (@geoffreyhinton) February 20, 2020 This post spurred large discussions about interpretable ML systems and whether explanation for their outputs are necessary. In this scenario, I believe that the scalability and cost will nudge the user’s decision, but that’s a huge topic appropriate for a future post of its own. As pointed out in many interpretability papers1 ML systems for performance does not address important criteria such as nondiscrimination, safety, or right to explanation by end-users or legal systems. Ignoring any of these criteria could lead to alarming consequences, but unlike measures of performance, these criteria are usually difficult to quantify. For example, HR system may discriminate applications based on many confounders that may not be obvious to the recruiter. To address such cases, ML researchers are developing methods to measure interpretability; or the ability to explain ML systems in understandable terms to human. . The attention mechanism in transformers has dramatically improved the performance in wide domains, and it also provides insight for interpretability. The main idea is this: each time the model generates an output, it only refers small part of the input that is most relevant to the output. By quantifying the level of input relevance, users can evaluate which part the data the model is most attentive to. For a more in-depth explanation, check out these great posts Attention? Attention! by Lilian Weng and The Annotated Transformer. Note that the transformers are a variant of attention model that revolutionized natural language processing in recent years. I will post other interesting interpretability probing techniques so stay tuned! . Acknowledgments . References terminologies from ‘Towards A Rigorous Science of Interpretable Machine Learning’ by Finale Doshi-Velez and Been Kim. &#8617; . |",
            "url": "https://repoofideas.github.io/blog/interpretable%20machine%20learning/transformers/2020/03/16/Case-for-Interpretable-Machine-Learning.html",
            "relUrl": "/interpretable%20machine%20learning/transformers/2020/03/16/Case-for-Interpretable-Machine-Learning.html",
            "date": " • Mar 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Problem Orientation for the Long Term",
            "content": "Problem Orientation for the Long Term . It has been a year since I have enrolled in a computer science graduate program. A lot has changed since then, including my rent price(rip my savings), but there’s one thing I am repeating exactly the same as last year at this period. I am applying to summer internships amid turndowns. Let’s look at a post from the subreddit r/cscareeradvice that describes how internship rejection truly feels. . What you feel, anytime you begin to think about them; That hurt you can’t exactly articulate into words? It’s okay to feel those feelings. It’s okay to hurt. If necessary, it’s okay to cry. You won’t be any less of a person because you’re experiencing strong emotions. . That was actually from r/datingadvice but you can see the analogy! Gratefully, I was able to land a full-time research internship last summer but there was one important lesson which I would like to instill in myself for an optimal, long-term research career. Last year, I was embarrassingly overconfident in few deep learning techniques that I have learned. I have recommended coworkers to apply those techniques without valid justification or understanding of their research problems. As amazing as deep learning is, prioritizing technique itself is short-sighted and easily misguided. . Early this year, I found a book written by Peter Feibelman called ’ A PhD is Not Enough,’ an alarming title for a mere master’s student. I want to quote a great excerpt about problem-orientation. . When a remarkable new instrument, such as the laser, or a technique, like nuclear magnetic resonance spectrometry, becomes available, it is often profitable to ask how its capabilities can be applied to solving outstanding problems. Few scientists, however, are able to make a long-term success of applying their favorite technique to one problem after another. Eventually the well runs dry. It is the researchers who focus on a significant problem and are willing to bring to it whatever resources are necessary who give the most absorbing talks, write the most significant papers, and win grant support most easily. I strongly recommend that you try to teach yourself to be problem-oriented, to plan your research projects so that they address important scientific issues regardless of what techniques you and your coworkers will need to use. . Looking at the number of papers coming out of deep learning community these days, learning the new SOTA technique will not be marketable for much. Without problem-orientation, it may be nearly impossible to survive in this rapidly evolving field and one must steer away from the juicy trap of technique-orientation. .",
            "url": "https://repoofideas.github.io/blog/research%20rants/2020/01/14/Problem-Orientation.html",
            "relUrl": "/research%20rants/2020/01/14/Problem-Orientation.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I have recently graduated with a master’s degree in computer science from Stony Brook University. My research at LUNR lab was focused on developing deep-learning-based aggregation mechanisms that can prevent inference from combining semantically incompatible evidence. I am also interested in the empirical side of designing large-scale experiments and evaluating the performances of language models. This research project was advised by Prof. Niranjan Balasubramanian and funded by NSF. I have previously worked on projects in the domains of computer vision and neuroscience. . If you want to read more about my past projects and experiences, please visit my Linkedin. . I also love to test with multi-modal and user-guided generation. The logo and the title font on my front page are generated by AI, and I have modified about 2%(~1200 pixels) of it manually. .",
          "url": "https://repoofideas.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://repoofideas.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}